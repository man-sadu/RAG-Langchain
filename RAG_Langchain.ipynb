{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VrqcQzGDvib"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai tiktoken faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import necessary libraries"
      ],
      "metadata": {
        "id": "RRstINEzDzQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import getpass"
      ],
      "metadata": {
        "id": "NWxZ1TPCDwm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OpenAI API Key"
      ],
      "metadata": {
        "id": "NAySmnDUD509"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"*********\"#Enter your API Key"
      ],
      "metadata": {
        "id": "Ro8kjIWvD1ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Providing Docs for retrieval"
      ],
      "metadata": {
        "id": "xRi5keK-EIQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "academic_text = \"\"\"\n",
        "LangChain is a framework designed to help developers build powerful applications that use large language models (LLMs).\n",
        "It provides tools for chaining different components like prompts, models, and memory for conversational systems.\n",
        "Retrieval-Augmented Generation (RAG) helps improve accuracy by combining external knowledge with LLM responses.\n",
        "Prompting techniques like Few-Shot Learning and Chain-of-Thought (CoT) reasoning can improve reasoning and factual accuracy.\n",
        "\"\"\"\n",
        "with open(\"academic_faq.txt\", \"w\") as f:\n",
        "    f.write(academic_text)"
      ],
      "metadata": {
        "id": "DevOy0VUEFek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Embeddings for Documents"
      ],
      "metadata": {
        "id": "OIP2UsxdERdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"academic_faq.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "split_docs = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "9olDWU0bEOBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(split_docs, embeddings)"
      ],
      "metadata": {
        "id": "ED7a9cleEWz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initializaing Memory"
      ],
      "metadata": {
        "id": "_90LxArUEcr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3),\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "P7UgL_WKEZ_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zero Shot prompting"
      ],
      "metadata": {
        "id": "klk4EBnQEiBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer this question as clearly as possible:\\nQuestion: {question}\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Qw-5uxWEEfEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Few shot prompting"
      ],
      "metadata": {
        "id": "_swieKKyEnw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_examples = \"\"\"\n",
        "Example 1:\n",
        "Q: What is LangChain?\n",
        "A: A framework for building applications using LLMs.\n",
        "\n",
        "Example 2:\n",
        "Q: What is RAG?\n",
        "A: Retrieval-Augmented Generation improves factual accuracy.\n",
        "\n",
        "Now, answer:\n",
        "Q: {question}\n",
        "\"\"\"\n",
        "few_shot_prompt = PromptTemplate(input_variables=[\"question\"], template=few_shot_examples)"
      ],
      "metadata": {
        "id": "S0aA7-IiEkLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chain of thought prompting"
      ],
      "metadata": {
        "id": "jumjBVwREsRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Think step by step before answering. Question: {question}\")"
      ],
      "metadata": {
        "id": "2EZzClKIEprb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_reflection_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=(\n",
        "        \"You are a reasoning agent. First, give an initial answer. Then, reflect and correct it if needed.\\n\"\n",
        "        \"Question: {question}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "L-RiRk7-Euow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initializing llm and langchain chains"
      ],
      "metadata": {
        "id": "g06x6AYiE04c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
        "\n",
        "zero_chain = LLMChain(llm=llm, prompt=zero_prompt)\n",
        "few_chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
        "cot_chain = LLMChain(llm=llm, prompt=cot_prompt)\n",
        "self_reflect_chain = LLMChain(llm=llm, prompt=self_reflection_prompt)"
      ],
      "metadata": {
        "id": "6M7jZURbExjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How does RAG improve chatbot accuracy?\"\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "print(zero_chain.run(question))\n",
        "print(\"\\nFew-shot:\")\n",
        "print(few_chain.run(question))\n",
        "print(\"\\nChain-of-Thought:\")\n",
        "print(cot_chain.run(question))\n",
        "print(\"\\nSelf-Reflection:\")\n",
        "print(self_reflect_chain.run(question))"
      ],
      "metadata": {
        "id": "erCbmkp-E46Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVBxGvEME-d1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}